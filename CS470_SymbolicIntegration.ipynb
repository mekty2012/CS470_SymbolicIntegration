{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470_SymbolicIntegration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oub16kLOqpRd"
      },
      "source": [
        "## 1 Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcYEOLiPqVgd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from itertools import product"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drz9A5Scqssz"
      },
      "source": [
        "## 2 Seed randoms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhlwaA3YqmqX"
      },
      "source": [
        "manual_seed = \"7777\".__hash__() % (2 ** 32) #random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "\n",
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35Ohs0mqwH0"
      },
      "source": [
        "token_list = [\"exp\", \"log\", \"sqrt\", \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"sinh\", \"cosh\", \"tanh\", \"asinh\", \"acosh\", \"atanh\",\n",
        "              \"+\", \"-\", \"*\", \"/\", \"x\", \"-5\", \"-4\", \"-3\", \"-2\", \"-1\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "def token_to_one_hot_encoding(token):\n",
        "    ind = token_list.index(token)\n",
        "    return [1 if i == ind else 0 for i in range(len(token_list))]\n",
        "\n",
        "def logit_to_token(prob_list):\n",
        "    return token_list[prob_list.index(max(prob_list))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcHTJJ8PbjS1"
      },
      "source": [
        "def preprocess_input(inp):\n",
        "    \"\"\"\n",
        "    Given batch of input, you should transform it to torch.Tensor.\n",
        "    Currently, the input is assumed to be tuple...?\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R53QKKkAxrOO"
      },
      "source": [
        "class ChildSumTreeLSTM(nn.Module):\n",
        "    def __init__(self, in_dim, cell_dim):\n",
        "        super(ChildSumTreeLSTM, self).__init__()\n",
        "        self.sigmoid = nn.sigmoid()\n",
        "        self.tanh = nn.tanh()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.cell_dim = cell_dim\n",
        "        \n",
        "        self.input_Wf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wf = nn.Linear(self.cell_dim, self.cell_dim, bias=False)\n",
        "        self.input_Wi = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wi = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "        self.input_Wo = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wo = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "        self.input_Wu = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wu = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "\n",
        "    def forward_internal(self, inp, hiddens = None, cells = None):\n",
        "        # inp : (batch_size, in_dim)\n",
        "        # hiddens : (batch_size, child_num, cell_dim)\n",
        "        # cells : (batch_size, child_num, cell_dim)\n",
        "        if hiddens is None:\n",
        "            hiddens = torch.zeros([inp.shape[0], 1, self.cell_dim])\n",
        "        if cells is None:\n",
        "            cells = torch.zeros([inp.shape[0], 1, self.cell_dim])\n",
        "\n",
        "        num_child = hiddens.shape[1]\n",
        "        new_hidden = torch.sum(hiddens, 1) # (batch_size, cell_dim)\n",
        "        \n",
        "        i_vec = self.sigmoid(self.input_Wi(inp) + self.hidden_Wi(new_hidden)) # (batch_size, cell_dim)\n",
        "        o_vec = self.sigmoid(eslf.input_Wo(inp) + self.hidden_Wo(new_hidden)) # (batch_size, cell_dim)\n",
        "        u_vec = self.tanh(eslf.input_Wu(inp) + self.hidden_Wu(new_hidden)) # (batch_size, cell_dim)\n",
        "\n",
        "        flat_hidden = hiddens.view(-1, self.cell_dim) # (batch_size * child_num, cell_dim)\n",
        "        input_f_vec = self.input_Wf(inp).repeat(1, num_child) # (batch_size, child_num, cell_dim)\n",
        "        hidden_f_vec = self.hidden_Wf(flat_hidden).view(-1, num_child, self.cell_dim) # (batch_size, child_num, cell_dim)\n",
        "        f_vec = self.sigmoid(input_f_vec + hidden_f_vec) # (batch_size, child_num, cell_dim)\n",
        "\n",
        "        c_vec = i_vec * u_vec + torch.sum(cells * f_vec, 1) # (batch_size, cell_dim)\n",
        "        h_vec = o_vec * self.tanh(c_vec) # (batch_size, cell_dim)\n",
        "\n",
        "        return c_vec, h_vec\n",
        "\n",
        "    def forward_internal(self, inp):\n",
        "        if inp.shape[1] == 3:\n",
        "            left_cell, left_hidden = forward(inp[:, 1])\n",
        "            right_cell, right_hidden = forward(inp[:, 2])\n",
        "            left_hidden = left_hidden.view(-1, 1, self.cell_dim)\n",
        "            right_hidden = right_hidden.view(-1, 1, self.cell_dim)\n",
        "            left_cell = left_cell.view(-1, 1, self.cell_dim)\n",
        "            right_cell = right_cell.view(-1, 1, self.cell_dim)\n",
        "            return forward_internal(inp[:, 0], torch.cat([left_hidden, right_hidden], 1), torch.cat([left_cell, right_cell], 1))\n",
        "        elif inp.shape[1] == 2:\n",
        "            cell, hidden = forward(inp[:, 1])\n",
        "            return forward_internal(inp[:, 0], hidden.view(-1, 1, self.cell_dim), cell.view(-1, 1, self.cell_dim))\n",
        "        else:\n",
        "            return forward_internal(inp[:, 0])\n",
        "\n",
        "    def forward(self, inp):\n",
        "        return self.forward_internal(inp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdxqhmmJBCac"
      },
      "source": [
        "class BinaryTreeLSTM(nn.Module):\n",
        "    def __init__(self, in_dim, cell_dim):\n",
        "        super(BinaryTreeLSTM, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.cell_dim = cell_dim\n",
        "\n",
        "        self.sigmoid = nn.sigmoid()\n",
        "        self.tanh = nn.tanh()\n",
        "\n",
        "        self.input_Wi = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wo = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wu = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wlf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wrf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "\n",
        "        self.left_Wi = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wo = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wu = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wlf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wrf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        \n",
        "        self.right_Wi = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wo = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wu = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wlf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wrf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "\n",
        "    def forward_internal(self, inp, left_hidden=None, left_cell=None, right_hidden=None, right_cell=None):\n",
        "        # inp : (batch_size, in_dim)\n",
        "        # hiddens : (batch_size, child_num, cell_dim)\n",
        "        # cells : (batch_size, child_num, cell_dim)\n",
        "        if left_hidden is None:\n",
        "            left_hidden = torch.zeros(inp.shape[0], self.in_dim, self.cell_dim)\n",
        "        if right_hidden is None:\n",
        "            right_hidden = torch.zeros(inp.shape[0], self.in_dim, self.cell_dim)\n",
        "        if left_cell is None:\n",
        "            left_cell = torch.zeros(inp.shape[0], self.in_dim, self.cell_dim)\n",
        "        if right_cell is None:\n",
        "            right_cell = torch.zeros(inp.shape[0], self.in_dim, self.cell_dim)\n",
        "        \n",
        "        i_vec = self.sigmoid(self.input_Wi(inp) + self.left_Wi(left_hidden) + self.right_Wi(right_hidden))\n",
        "        o_vec = self.sigmoid(self.input_Wo(inp) + self.left_Wo(left_hidden) + self.right_Wo(right_hidden))\n",
        "        u_vec = self.tanh(self.input_Wu(inp) + self.left_Wu(left_hidden) + self.right_Wu(right_hidden))\n",
        "        left_f_vec = self.sigmoid(self.input_Wlf(inp) + self.left_Wlf(left_hidden) + self.right_Wlf(right_hidden))\n",
        "        right_f_vec = self.sigmoid(self.input_Wrf(inp) + self.left_Wrf(left_hidden) + self.right_Wrf(right_hidden))\n",
        "\n",
        "        c_vec = i_vec * u_vec + left_f_vec * left_cell + right_f_vec * right_cell\n",
        "        h_vec = o_vec * self.tanh(c_vec)\n",
        "\n",
        "        return c_vec, h_vec\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if inp.shape[1] == 3:\n",
        "            left_cell, left_hidden = forward(inp[:, 1])\n",
        "            right_cell, right_hidden = forward(inp[:, 2])\n",
        "            return forward_internal(inp[:, 0], left_hidden, left_cell, right_hidden, right_cell)\n",
        "        elif inp.shape[1] == 2:\n",
        "            cell, hidden = forward(inp[:, 1])\n",
        "            return forward_internal(inp[:, 0], hidden, cell)\n",
        "        else:\n",
        "            return forward_internal(inp[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8rsLbaPJjSm"
      },
      "source": [
        "class RecursiveNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super(RecuriveNN, self).__init__()\n",
        "        self.dim = hidden_dim\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.inp_linear = nn.Linear(self.in_dim, self.hidden_dim)\n",
        "        self.left_linear = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "        self.right_linear = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        if inp.shape[1] == 3:\n",
        "            left = forward(inp[:, 1])\n",
        "            right = forward(inp[:, 2])\n",
        "            return self.activation(self.inp_linear(inp[:, 0]) + self.left_linear(left) + self.right_linear(right))\n",
        "        elif inp.shape[1] == 2:\n",
        "            left = forward(inp[:, 1])\n",
        "            return self.activation(self.inp_linear(inp[:, 0]) + self.left_linear(left)\n",
        "        else:\n",
        "            return self.activation(self.inp_linear(inp[:, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlBnKR8mNZUi"
      },
      "source": [
        "class CompositionalSemantics(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(CompositionalSemantics, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.unary_ops = {\n",
        "            \"exp\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"log\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sqrt\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sin\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"cos\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"tan\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"asin\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"acos\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"atan\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sinh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"cosh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"tanh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"asinh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"acosh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"atanh\" : nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        }\n",
        "        self.binary_ops = {\n",
        "            \"+\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"-\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"*\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"/\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
        "        }\n",
        "        self.terminals = {\n",
        "            \"x\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-5\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-4\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-3\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-2\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-1\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"1\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"2\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"3\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"4\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"5\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "        }\n",
        "    \n",
        "    def forward_internal(self, inp):\n",
        "        if inp.shape[1] == 3:\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Assumes input is given as tree\"\"\"\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qtQ2i3yCS2y"
      },
      "source": [
        "class TBCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TBCNN, self).__init__()\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8b8KAL-hswD"
      },
      "source": [
        "# Code2Seq will use RNN/LSTM/... structures, so we don't need module. We need to define preprocess function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migtrkRzh3rM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}