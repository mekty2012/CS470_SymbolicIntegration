{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470_SymbolicIntegration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7RDVoDdvShbo8dj0NvPHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mekty2012/CS470_SymbolicIntegration/blob/main/CS470_SymbolicIntegration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oub16kLOqpRd"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcYEOLiPqVgd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch\n",
        "\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from itertools import product"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drz9A5Scqssz"
      },
      "source": [
        "## 2. Seed randoms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhlwaA3YqmqX"
      },
      "source": [
        "manual_seed = \"7777\".__hash__() % (2 ** 32) #random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "\n",
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0mjin3B19mG"
      },
      "source": [
        "## 3. Implement preprocessing / postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35Ohs0mqwH0"
      },
      "source": [
        "token_list = [\"exp\", \"log\", \"sqrt\", \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"sinh\", \"cosh\", \"tanh\", \"asinh\", \"acosh\", \"atanh\",\n",
        "              \"+\", \"-\", \"*\", \"/\", \"x\", \"-5\", \"-4\", \"-3\", \"-2\", \"-1\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "def token_to_one_hot_encoding(token):\n",
        "    ind = token_list.index(token)\n",
        "    return [1 if i == ind else 0 for i in range(len(token_list))]\n",
        "\n",
        "def logit_to_token(prob_list):\n",
        "    return token_list[prob_list.index(max(prob_list))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n8_8iQekpoI"
      },
      "source": [
        "## 4. Implement models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4R-DInW2CAM"
      },
      "source": [
        "### 1. ChildSumLSTM\n",
        "\n",
        "Given inp $v$, hiddens $h_i$, cells $c_i$, computes\n",
        "\n",
        "$h = \\sum_{i=1}^C h_i$\n",
        "\n",
        "$v_i = \\sigma(I_i(v) + H_i(h))$\n",
        "\n",
        "$v_o = \\sigma(I_o(v) + H_o(h)$\n",
        "\n",
        "$v_u = \\tanh(I_u(v) + H_u(h)$\n",
        "\n",
        "$v_{f}^i = \\sigma(I_f(v) + h_i)$\n",
        "\n",
        "$v_c = v_i + v_u + \\sum_{i=1}^C v_{f}^i * c_i$\n",
        "\n",
        "$v_h = v_o * \\tanh(v_c)$\n",
        "\n",
        "You can stack ChildSumLSTM by letting parameter `return_sequences = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R53QKKkAxrOO"
      },
      "source": [
        "def postorder_checker(depth):\n",
        "    if depth == 1:\n",
        "        return [True]\n",
        "    else:\n",
        "        return postorder_checker(depth - 1) * 2 + [False]\n",
        "\n",
        "class ChildSumTreeLSTM(nn.Module):\n",
        "    def __init__(self, in_dim, cell_dim, return_sequences = False):\n",
        "        super(ChildSumTreeLSTM, self).__init__()\n",
        "        self.return_sequences = return_sequences\n",
        "        self.sigmoid = nn.sigmoid()\n",
        "        self.tanh = nn.tanh()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.cell_dim = cell_dim\n",
        "        \n",
        "        self.input_Wf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wf = nn.Linear(self.cell_dim, self.cell_dim, bias=False)\n",
        "        self.input_Wi = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wi = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "        self.input_Wo = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wo = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "        self.input_Wu = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.hidden_Wu = nn.Linear(self.cell_dim, self.cell_dim, bias = False)\n",
        "        \n",
        "        \n",
        "    def forward(self, inps):\n",
        "        # Assumes that input is given as postorder traversal.\n",
        "        # inps : (batch_size, node_num, input_dim)\n",
        "        batch_size = inps.shape[0]\n",
        "        if self.return_sequences:\n",
        "            result_seq = None\n",
        "        stack = []\n",
        "        size = inps.shape[1] # Get number of nodes\n",
        "        depth = size.bit_length()\n",
        "        checker = postorder_checker(depth)\n",
        "        for i, tf in enumerate(checker):\n",
        "            inp = inps[:, i, :]\n",
        "            if tf: # external node\n",
        "                hiddens = torch.zeros(batch_size, 2, self.cell_dim])\n",
        "                cells = torch.zeros(batch_size, 2, self.cell_dim])\n",
        "                new_hidden = torch.zeros(batch_size, 1, self.cell_dim])\n",
        "            else:\n",
        "                assert len(stack) >= 2\n",
        "                left_hidden, left_cell = stack.pop()\n",
        "                right_hidden, right_cell = stack.pop()\n",
        "                new_hidden = torch.sum(left_hidden, right_hidden)\n",
        "                \n",
        "            i_vec = self.sigmoid(self.input_Wi(inp) + self.hidden_Wi(new_hidden)) # (batch_size, cell_dim)\n",
        "            o_vec = self.sigmoid(eslf.input_Wo(inp) + self.hidden_Wo(new_hidden)) # (batch_size, cell_dim)\n",
        "            u_vec = self.tanh(eslf.input_Wu(inp) + self.hidden_Wu(new_hidden)) # (batch_size, cell_dim)\n",
        "\n",
        "            flat_hidden = torch.cat([left_hidden, right_hidden], dim=1)\n",
        "            input_f_vec = self.input_Wf(inp).repeat(1, 2)\n",
        "            hidden_f_vec = self.hidden_Wf(flat_hidden).view(-1, 2, self.cell_dim)\n",
        "            f_vec = self.sigmoid(input_f_vec + hidden_f_vec)\n",
        "               \n",
        "            c_vec = i_vec * u_vec + torch.sum(cells * f_vec, 1)\n",
        "            h_vec = o_vec * self.tanh(c_vec)\n",
        "\n",
        "            stack.append((h_vec, c_vec))\n",
        "            if self.return_sequences:\n",
        "                if return_seq is None:\n",
        "                    return_seq = c_vec\n",
        "                else:\n",
        "                    return_seq = torch.cat([return_seq, c_vec], dim=1)\n",
        "        assert len(stack) == 1\n",
        "        if self.return_sequences:\n",
        "            return return_seq\n",
        "        else:\n",
        "            return stack[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfrmV4m3ie_"
      },
      "source": [
        "### 2. Binary Tree LSTM\n",
        "\n",
        "Given inp $v$, hiddens $h_l, h_r$, cells $c_l, c_r$, computes\n",
        "\n",
        "$v_i = \\sigma(I_i(v) + H_i^l(h_l) + H_i^r(h_r)$\n",
        "\n",
        "$v_u = \\sigma(I_u(v) + H_u^l(h_l) + H_u^r(h_r)$\n",
        "\n",
        "$v_o = \\tanh(I_o(v) + H_o^l(h_l) + H_o^r(h_r)$\n",
        "\n",
        "$v_{f}^l = \\sigma(I_{lf}^l(v) + H_{lf}^l(h_l) + H_{lf}^l(h_r)$\n",
        "\n",
        "$v_{f}^r = \\sigma(I_{rf}^l(v) + H_{rf}^l(h_l) + H_{rf}^l(h_r)$\n",
        "\n",
        "$v_c = v_i + v_u + v_f^l * c_l + v_f^r * c_r$\n",
        "\n",
        "$v_h = v_o * \\tanh(v_c)$\n",
        "\n",
        "You can stack BinaryTreeLSTM by letting parameter `return_sequences = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdxqhmmJBCac"
      },
      "source": [
        "class BinaryTreeLSTM(nn.Module):\n",
        "    def __init__(self, in_dim, cell_dim, return_sequences = False):\n",
        "        super(BinaryTreeLSTM, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.cell_dim = cell_dim\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "        self.sigmoid = nn.sigmoid()\n",
        "        self.tanh = nn.tanh()\n",
        "\n",
        "        self.input_Wi = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wo = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wu = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wlf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "        self.input_Wrf = nn.Linear(self.in_dim, self.cell_dim)\n",
        "\n",
        "        self.left_Wi = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wo = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wu = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wlf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.left_Wrf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        \n",
        "        self.right_Wi = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wo = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wu = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wlf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "        self.right_Wrf = nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
        "\n",
        "    def forward(self, inps):\n",
        "        batch_size = inps.shape[0]\n",
        "        if self.return_sequences:\n",
        "            result_seq = None\n",
        "        stack = []\n",
        "        size = inps.shape[1] # Get number of nodes\n",
        "        depth = size.bit_length()\n",
        "        checker = postorder_checker(depth)\n",
        "        for i, tf in enumerate(checker):\n",
        "            if tf:\n",
        "                left_hidden = torch.zeros(batch_size, self.cell_dim)\n",
        "                right_hidden = torch.zeros(batch_size, self.cell_dim)\n",
        "                left_cell = torch.zeros(batch_size, self.cell_dim)\n",
        "                right_cell = torch.zeros(batch_size, self.cell_dim)\n",
        "            else:\n",
        "                right_hidden, right_cell = stack.pop()\n",
        "                left_hidden, left_cell = stack.pop()\n",
        "            \n",
        "            i_vec = self.sigmoid(self.input_Wi(inp) + self.left_Wi(left_hidden) + self.right_Wi(right_hidden))\n",
        "            o_vec = self.sigmoid(self.input_Wo(inp) + self.left_Wo(left_hidden) + self.right_Wo(right_hidden))\n",
        "            u_vec = self.tanh(self.input_Wu(inp) + self.left_Wu(left_hidden) + self.right_Wu(right_hidden))\n",
        "            left_f_vec = self.sigmoid(self.input_Wlf(inp) + self.left_Wlf(left_hidden) + self.right_Wlf(right_hidden))\n",
        "            right_f_vec = self.sigmoid(self.input_Wrf(inp) + self.left_Wrf(left_hidden) + self.right_Wrf(right_hidden))\n",
        "\n",
        "            c_vec = i_vec * u_vec + left_f_vec * left_cell + right_f_vec * right_cell\n",
        "            h_vec = o_vec * self.tanh(c_vec)\n",
        "\n",
        "            stack.append((h_vec, c_vec))\n",
        "            if self.return_sequences:\n",
        "                if result_seq is None:\n",
        "                    result_seq = c_vec\n",
        "                else:\n",
        "                    result_seq = torch.cat(result_seq, c_vec)\n",
        "        \n",
        "        assert len(stack) == 1\n",
        "        if self.return_sequences:\n",
        "            return result_seq\n",
        "        else:\n",
        "            return stack[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAIC14qi3ShB"
      },
      "source": [
        "### 3. Recursive Neural Network\n",
        "\n",
        "Given input $v_i$, left $v_l$, right $v_r$, computes\n",
        "\n",
        "$v = act(L_i(v_i) + L_l(v_l) + L_r(v_r))$\n",
        "\n",
        "You can stack RecursiveNN by letting parameter `return_sequences = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8rsLbaPJjSm"
      },
      "source": [
        "class RecursiveNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, return_sequences = False):\n",
        "        super(RecuriveNN, self).__init__()\n",
        "        self.dim = hidden_dim\n",
        "        self.activation = nn.ReLU()\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "        self.inp_linear = nn.Linear(self.in_dim, self.hidden_dim)\n",
        "        self.left_linear = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "        self.right_linear = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "    \n",
        "    def forward(self, inps):\n",
        "        batch_size = inps.shape[0]\n",
        "        if self.return_sequences:\n",
        "            result_seq = None\n",
        "        stack = []\n",
        "        size = inps.shape[1] # Get number of nodes\n",
        "        depth = size.bit_length()\n",
        "        checker = postorder_checker(depth)\n",
        "        for i, tf in enumerate(checker):\n",
        "            inp = inps[:, i, :]\n",
        "            if tf:\n",
        "                right = stack.pop()\n",
        "                left = stack.pop()\n",
        "                res = self.activation(self.inp_linear(inp) + self.left_linear(left) + self.right_linear(right))\n",
        "                stack.append(res)\n",
        "                if self.return_sequences:\n",
        "                    if result_seq is None:\n",
        "                        result_seq = res\n",
        "                    else:\n",
        "                        result_seq = torch.cat(result_seq, res)\n",
        "            else:\n",
        "                res = stack.append(self.activation(self.inp_linear(inp)))\n",
        "                if self.return_sequences:\n",
        "                    if result_seq is None:\n",
        "                        result_seq = res\n",
        "                    else:\n",
        "                        result_seq = torch.cat(result_seq, res)\n",
        "        assert len(stack) == 1\n",
        "        if self.return_sequences:\n",
        "            return return_seq\n",
        "        else:\n",
        "            return stack[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcD9i8hy4iXv"
      },
      "source": [
        "### 4. Compositional Semantics\n",
        "\n",
        "Compose the opeartors.\n",
        "\n",
        "Each unary ops corresponds to $n \\times n$ matrix, binary ops corresponds to $2n \\times n$ matrix, terminal corresponds to length $n$ vector.\n",
        "\n",
        "You can not stack this module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlBnKR8mNZUi"
      },
      "source": [
        "class CompositionalSemantics(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(CompositionalSemantics, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.unary_ops = nn.ModuleDict({\n",
        "            \"exp\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"log\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sqrt\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sin\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"cos\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"tan\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"asin\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"acos\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"atan\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"sinh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"cosh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"tanh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"asinh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"acosh\" : nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "            \"atanh\" : nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        })\n",
        "        \n",
        "        self.binary_ops = nn.ModuleDict({\n",
        "            \"+\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"-\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"*\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
        "            \"/\" : nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
        "        })\n",
        "        self.terminals = nn.ModuleDict({\n",
        "            \"x\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-5\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-4\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-3\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-2\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"-1\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"1\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"2\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"3\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"4\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "            \"5\" : torch.rand(self.hidden_dim, requires_grad = True),\n",
        "        })\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Assumes input is given as postorder traversal\"\"\"\n",
        "        stack = []\n",
        "        for i, curr in enumerate(inp):\n",
        "            if curr in self.unary_ops:\n",
        "                param = stack.pop()\n",
        "                func = self.unary_ops[curr]\n",
        "                stack.append(func(param))\n",
        "            elif curr in self.binary_ops:\n",
        "                right_param = stack.pop()\n",
        "                left_param = stack.pop()\n",
        "                func = self.binary_ops[curr]\n",
        "                stack.append(func(torch.cat([left_param, right_param], dim=1)))\n",
        "            elif curr in self.terminals:\n",
        "                stack.append(self.terminals[curr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMeEf8_445kG"
      },
      "source": [
        "### 5. Code2Seq\n",
        "\n",
        "Sample the paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8b8KAL-hswD"
      },
      "source": [
        "# Code2Seq will use RNN/LSTM/... structures, so we don't need module. We need to define preprocess function.\n",
        "def code2seq_sample(tree, num):\n",
        "    results = []\n",
        "    terminals = list_terminal(tree)\n",
        "    if num == -1:\n",
        "        for i in range(len(terminals)):\n",
        "            for j in range(i + 1, len(terminals)):\n",
        "                left = terminals[i]\n",
        "                right = terminals[j]\n",
        "                results.append(terminal_to_path(tree, left, right))\n",
        "        # Sample all paths\n",
        "    else:\n",
        "        l = []\n",
        "        for i in range(len(terminals)):\n",
        "            for j in range(i + 1, len(terminals)):\n",
        "                l.append((i, j))\n",
        "        indices = random.choices(l, k=num)\n",
        "        for index in indices:\n",
        "            left = terminals[i]\n",
        "            right = terminals[j]\n",
        "            results.append(terminal_to_path(tree, left, right))\n",
        "        # Sample n paths\n",
        "    return results\n",
        "\n",
        "def terminal_to_path(tree, first, second):\n",
        "    root = tree\n",
        "    for i in range(len(first)):\n",
        "        if first[i] == second[i]:\n",
        "            if first[i]:\n",
        "                root = root[1]\n",
        "            else:\n",
        "                root = root[2]\n",
        "        else:\n",
        "            break\n",
        "    # i is current index of first/second\n",
        "    left_index = []\n",
        "    left_root = root\n",
        "\n",
        "    for j in range(i, len(first)):\n",
        "        if first[j]:\n",
        "            left_root = left_root[1]\n",
        "        else:\n",
        "            left_root = left_root[2]\n",
        "        left_index.append(left_root[0])\n",
        "    \n",
        "    right_index = []\n",
        "    right_root = root\n",
        "    \n",
        "    for j in range(i, len(second)):\n",
        "        if first[j]:\n",
        "            right_root = right_root[1]\n",
        "        else:\n",
        "            right_root = right_root[2]\n",
        "        right_index.append(right_root[0])\n",
        "\n",
        "    return list(left_index.reverse()) + [root[0]] + right_index\n",
        "\n",
        "# Returns list of terminals. The terminal's position is encoded as True(left), False(right). If unary, use True.\n",
        "def list_terminal(tree):\n",
        "    if len(tree) == 1:\n",
        "        return [[]]\n",
        "    elif len(tree) == 2:\n",
        "        l = list_terminal(tree[1])\n",
        "        return [t + [True] for t in l]\n",
        "    elif len(tree) == 3:\n",
        "        l = list_terminal(tree[1])\n",
        "        l2 = list_terminal(tree[2])\n",
        "        return [t + [True] for t in l] + [t + [False] for t in l2]\n",
        "    else:\n",
        "        raise ValueError(\"Tree is expected to be at most binary.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl6XdXMNkafA"
      },
      "source": [
        "### 6. TBCNN\n",
        "\n",
        "Due to our tree structure (at most binary, windows size 2), TBCNN's implementation is identical to recursive NN. So we do not implement TBCNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2A-u7mGkr0X"
      },
      "source": [
        "## 5. Implement Training"
      ]
    }
  ]
}